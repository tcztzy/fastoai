# Generated by FastOAI, DON'T EDIT
from datetime import datetime
from typing import TYPE_CHECKING, Annotated, ClassVar, Literal

from openai.types.beta.assistant import ToolResources
from openai.types.beta.assistant_response_format_option import (
    AssistantResponseFormatOption,
)
from openai.types.beta.assistant_tool import AssistantTool
from pydantic import field_serializer
from sqlmodel import Field, Relationship

from .._metadata import WithMetadata
from .._types import as_sa_type
from .._utils import now, random_id_with_prefix

if TYPE_CHECKING:
    from .message import Message
    from .run import Run
    from .run_step import Step


class Assistant(WithMetadata, table=True):
    id: Annotated[str, Field(primary_key=True, default_factory=random_id_with_prefix("asst_"))]
    """The identifier, which can be referenced in API endpoints."""

    created_at: Annotated[datetime, Field(default_factory=now)]
    """The Unix timestamp (in seconds) for when the assistant was created."""

    description: str | None = None
    """The description of the assistant. The maximum length is 512 characters."""

    instructions: str | None = None
    """The system instructions that the assistant uses.

    The maximum length is 256,000 characters.
    """

    model: str
    """ID of the model to use.

    You can use the
    [List models](https://platform.openai.com/docs/api-reference/models/list) API to
    see all of your available models, or see our
    [Model overview](https://platform.openai.com/docs/models) for descriptions of
    them.
    """

    name: str | None = None
    """The name of the assistant. The maximum length is 256 characters."""

    object: ClassVar[Literal["assistant"]] = "assistant"
    """The object type, which is always `assistant`."""

    tools: Annotated[list[AssistantTool], Field(sa_type=as_sa_type(list[AssistantTool]))]
    """A list of tool enabled on the assistant.

    There can be a maximum of 128 tools per assistant. Tools can be of types
    `code_interpreter`, `file_search`, or `function`.
    """

    response_format: Annotated[AssistantResponseFormatOption | None, Field(sa_type=as_sa_type(AssistantResponseFormatOption), nullable=True)] = None
    """Specifies the format that the model must output.

    Compatible with [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),
    [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4),
    and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.

    Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
    Outputs which ensures the model will match your supplied JSON schema. Learn more
    in the
    [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).

    Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
    message the model generates is valid JSON.

    **Important:** when using JSON mode, you **must** also instruct the model to
    produce JSON yourself via a system or user message. Without this, the model may
    generate an unending stream of whitespace until the generation reaches the token
    limit, resulting in a long-running and seemingly "stuck" request. Also note that
    the message content may be partially cut off if `finish_reason="length"`, which
    indicates the generation exceeded `max_tokens` or the conversation exceeded the
    max context length.
    """

    temperature: float | None = None
    """What sampling temperature to use, between 0 and 2.

    Higher values like 0.8 will make the output more random, while lower values like
    0.2 will make it more focused and deterministic.
    """

    tool_resources: Annotated[ToolResources | None, Field(sa_type=as_sa_type(ToolResources), nullable=True)] = None
    """A set of resources that are used by the assistant's tools.

    The resources are specific to the type of tool. For example, the
    `code_interpreter` tool requires a list of file IDs, while the `file_search`
    tool requires a list of vector store IDs.
    """

    top_p: float | None = None
    """
    An alternative to sampling with temperature, called nucleus sampling, where the
    model considers the results of the tokens with top_p probability mass. So 0.1
    means only the tokens comprising the top 10% probability mass are considered.

    We generally recommend altering this or temperature but not both.
    """

    @field_serializer("created_at")
    def serialize_datetime(self, dt: datetime, _) -> int:
        return int(dt.timestamp())

    messages: list["Message"] = Relationship(back_populates="assistant")
    runs: list["Run"] = Relationship(back_populates="assistant")
    steps: list["Step"] = Relationship(back_populates="assistant")
