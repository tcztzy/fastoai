import importlib.metadata
import inspect
import re
from contextlib import asynccontextmanager
from pathlib import Path

import fsspec
import yaml
from fastapi import FastAPI

try:
    __version__ = importlib.metadata.version(__name__)
except importlib.metadata.PackageNotFoundError:
    __version__ = "1.0.0"

from .routers import router
from .settings import settings

if settings.generate_requests:
    from openai.types.beta.assistant_create_params import AssistantCreateParams
    from openai.types.beta.assistant_update_params import AssistantUpdateParams
    from openai.types.beta.thread_create_params import ThreadCreateParams
    from openai.types.beta.threads.message_create_params import MessageCreateParams
    from openai.types.beta.threads.run_create_params import RunCreateParamsBase
    from openai.types.chat.completion_create_params import CompletionCreateParamsBase

    HEADER = """\
# Generated by fastoai, DO NOT EDIT
from typing import Literal

from openai.types.beta.assistant import ToolResources
from openai.types.beta.assistant_response_format_option_param import (
    AssistantResponseFormatOptionParam,
)
from openai.types.beta.assistant_tool_choice_option_param import (
    AssistantToolChoiceOptionParam,
)
from openai.types.beta.assistant_tool_param import AssistantToolParam
from openai.types.beta.thread_create_params import Message
from openai.types.beta.threads.message_content_part_param import MessageContentPartParam
from openai.types.beta.threads.message_create_params import Attachment
from openai.types.beta.threads.run_create_params import (
    AdditionalMessage,
    TruncationStrategy,
)
from openai.types.beta.threads.runs.run_step_include import RunStepInclude
from openai.types.chat.chat_completion_audio_param import ChatCompletionAudioParam
from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam
from openai.types.chat.chat_completion_modality import ChatCompletionModality
from openai.types.chat.chat_completion_prediction_content_param import (
    ChatCompletionPredictionContentParam,
)
from openai.types.chat.chat_completion_stream_options_param import (
    ChatCompletionStreamOptionsParam,
)
from openai.types.chat.chat_completion_tool_choice_option_param import (
    ChatCompletionToolChoiceOptionParam,
)
from openai.types.chat.chat_completion_tool_param import ChatCompletionToolParam
from openai.types.chat.completion_create_params import (
    Function,
    FunctionCall,
    ResponseFormat,
)
from openai.types.shared_params import ResponseFormatText
from pydantic import BaseModel
"""
    body = []

    for params in (
        AssistantCreateParams,
        AssistantUpdateParams,
        ThreadCreateParams,
        MessageCreateParams,
        RunCreateParamsBase,
        CompletionCreateParamsBase,
    ):
        src = inspect.getsource(params)
        src = re.sub(
            r"class ([A-Z]\w+)\(TypedDict, total=False\):",
            r"class \1(BaseModel):",
            src,
        )
        src = re.sub(r"Required\[\s*(.+)\s*\]", r"\1", src, flags=re.MULTILINE)
        src = re.sub(r"List\[(.+?)\]", r"list[\1]", src)
        src = re.sub(r"Dict\[(.+?)\]", r"dict[\1]", src)
        src = re.sub(r"Iterable\[(.+)\]", r"list[\1]", src)
        src = re.sub(r"Optional\[(.+)\]", r"\1 | None = None", src)
        match params.__name__:
            case "AssistantCreateParams":
                src = re.sub(
                    r'model: [\s\S]+?"""',
                    'model: str\n    """',
                    src,
                    flags=re.MULTILINE,
                )
            case "AssistantUpdateParams":
                src = re.sub("tools: .+", r"\g<0> = []", src)
            case "MessageCreateParams":
                src = re.sub(
                    "content: .+", "content: str | list[MessageContentPartParam]", src
                )
            case "ThreadCreateParams":
                src = re.sub("messages: .+", r"\g<0> = []", src)
            case "RunCreateParamsBase":
                src = re.sub("RunCreateParamsBase", "RunCreateParams", src)
                src = re.sub(
                    r'model: [\s\S]+?"""',
                    'model: str\n    """',
                    src,
                    flags=re.MULTILINE,
                )
                src = re.sub(
                    "parallel_tool_calls: .+", "parallel_tool_calls: bool = True", src
                )
                src += "\n    stream: bool = False\n"
            case "CompletionCreateParamsBase":
                src = re.sub(
                    "CompletionCreateParamsBase", "CompletionCreateParams", src
                )
                src = re.sub(
                    "messages: .+", "messages: list[ChatCompletionMessageParam]", src
                )
                src = re.sub("model: .+", "model: str", src)
                src = re.sub(
                    "parallel_tool_calls: .+", "parallel_tool_calls: bool = True", src
                )
                src = re.sub(
                    "response_format: .+", r"\g<0> = ResponseFormatText()", src
                )
                src = re.sub("tool_choice: .+", r"\g<0> | None = None", src)
                src = re.sub("tools: .+", r"\g<0> = []", src)
                src = re.sub("function_call: .+", r"\g<0> | None = None", src)
                src = re.sub("functions: .+", r"\g<0> = []", src)
                src = re.sub("user: str", "user: str | None = None", src)
                src = re.sub(r"stop: .+", "stop: str | list[str] | None = None", src)
                src += """
    stream: bool = False
    \"""If set, partial message deltas will be sent, like in ChatGPT.

    Tokens will be sent as data-only
    [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
    as they become available, with the stream terminated by a `data: [DONE]`
    message.
    [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
    \"""

    def model_post_init(self, __context):
        if self.function_call is None:
            self.function_call = "auto" if len(self.functions) > 0 else "none"
        if self.tool_choice is None:
            self.tool_choice = "auto" if len(self.tools) > 0 else "none"
"""
        body.append(src)

    with Path(__file__).parent.joinpath("requests.py").open("w", encoding="utf-8") as f:
        f.write("\n\n".join([HEADER] + body))


@asynccontextmanager
async def lifespan(app: FastAPI):
    yield
    await settings.session.close()


app = FastAPI(title="FastOAI", version=__version__, lifespan=lifespan)
app.include_router(router)
with fsspec.open(
    "filecache::github://openai:openai-openapi@master/openapi.yaml",
    filecache={"cache_storage": str(Path(__file__).parent)},
) as f:
    schema = yaml.load(f, Loader=yaml.SafeLoader)
original_schema = app.openapi()
schema["info"] = original_schema["info"]
del schema["servers"]
app.openapi_schema = schema
